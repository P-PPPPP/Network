{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy;\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation and loss function;\n",
    "def func(f,x,y=None):\n",
    "    if f == 'relu':\n",
    "        return relu(x);\n",
    "    if f == 'softmax':\n",
    "        x = x.T - np.max(x,axis=1).T;\n",
    "        x = x.T\n",
    "        # softmax 自变量减去常量等同于分数上下同除相同常量\n",
    "        # 使得自变量尽量是负数，防止溢出\n",
    "        return softmax(x);\n",
    "    if f == 'sigmoid':\n",
    "        return sigmoid(x);\n",
    "    if f == 'x':\n",
    "        return x;\n",
    "    if f == 'MSE':\n",
    "        return 1/x.shape[0] * 1/2* np.sum((x - y)**2);\n",
    "    if f == 'Cross_Entropy':\n",
    "        res = - np.sum(y*np.log(x),axis=1);\n",
    "        return 1/x.shape[0] * np.sum(res);\n",
    "    if f == 'Real_Cross_Entropy':\n",
    "        res = - np.sum(y*np.log(x)+(1-y)*np.log(1-x),axis=1);\n",
    "        return 1/x.shape[0] * np.sum(res);\n",
    "def gra_func(f,x,y=None):\n",
    "    if f == 'relu':\n",
    "        return gra_relu(x);\n",
    "    if f =='softmax':\n",
    "        return gra_softmax(x);\n",
    "    if f == 'sigmoid':\n",
    "        return gra_sigmoid(x);\n",
    "    if f == 'x':\n",
    "        return 1;\n",
    "    if f == 'MSE':\n",
    "        return (x-y);\n",
    "    if f == 'Cross_Entropy':\n",
    "        return -y/x;\n",
    "    if f == 'Real_Cross_Entropy':\n",
    "        return -(y/x-(1-y)/(1-x));\n",
    "def relu(x):\n",
    "    res = np.abs(x);\n",
    "    res = (res + x)/2;\n",
    "    return res;\n",
    "def softmax(x):\n",
    "    x = x.T - np.max(x,axis=1).T;\n",
    "    x = x.T\n",
    "    out = np.exp(x).T/np.sum(np.exp(x),axis=1).T\n",
    "    out = out.T\n",
    "    return out;\n",
    "def sigmoid(x):\n",
    "    r1 = 1/(1+np.exp(-relu(x)));\n",
    "    r2 = np.exp(-relu(-x))/(1+np.exp(-relu(-x)));\n",
    "    return (r1+r2-1/2);\n",
    "def gra_relu(x):\n",
    "    res = np.abs(x);\n",
    "    res = np.sign(res + x);\n",
    "    return res;\n",
    "def gra_softmax(x):\n",
    "    x = x.T - np.max(x,axis=1).T;\n",
    "    x = x.T\n",
    "    out = np.exp(x).T/np.sum(np.exp(x),axis=1).T\n",
    "    out = out.T\n",
    "    return (out - out**2);\n",
    "def gra_sigmoid(x):\n",
    "    r1 = np.exp(-relu(x))/(1+np.exp(-relu(x)))**2;\n",
    "    r2 = np.exp(-relu(-x))/(1+np.exp(-relu(-x)))**2;\n",
    "    return (r1+r2-1/4);\n",
    "# 训练集随机化及从训练集产生测试集\n",
    "def random_division(x,y=None,test_rate = 0):\n",
    "    x = np.array(x);\n",
    "    # 数据总长度\n",
    "    n = np.shape(x)[0];\n",
    "    # 生成下表序列并随即打乱\n",
    "    sequence = np.array(range(n));\n",
    "    np.random.shuffle(sequence);\n",
    "    # 测试集长度\n",
    "    n_test = int(np.ceil(n * test_rate));\n",
    "    # 测试集序列与训练集序列\n",
    "    se_test = sequence[:n_test];\n",
    "    se_train = sequence[n_test:];\n",
    "    # 训练集\n",
    "    train_data = x[se_train];\n",
    "    # 测试集\n",
    "    test_data = x[se_test];\n",
    "    # 有监督学习的情况\n",
    "    if not type(y) == type(None):\n",
    "        y = np.array(y);\n",
    "        train_label = y[se_train];\n",
    "        test_label = y[se_test];\n",
    "        if not test_rate == 0:\n",
    "            return (train_data,train_label,test_data,test_label);\n",
    "        else:\n",
    "            return (train_data,train_label);\n",
    "    # 无监督情况\n",
    "    if not test_rate == 0:\n",
    "        return (train_data,test_data);\n",
    "    else:\n",
    "        return (train_data);\n",
    "# label to one-hot\n",
    "def onehot_label(label):\n",
    "    max_num = np.max(label);\n",
    "    res = [];\n",
    "    for k in range(len(label)):\n",
    "        resk = np.zeros((max_num+1,));\n",
    "        resk[label[k]] = 1;\n",
    "        res.append(resk);\n",
    "    return np.array(res);\n",
    "def arg_max(y):\n",
    "    tar = np.argmax(y,axis=1);\n",
    "    res = np.zeros_like(y);\n",
    "    for i in range(y.shape[0]):\n",
    "        res[i,tar[i]] = 1;\n",
    "    return res;\n",
    "def evaluate(y,label,metrics='Accuracy'):\n",
    "    confusion_matrix = np.dot(label.T,y);\n",
    "    cm = confusion_matrix;\n",
    "    # 对角线元素+1 防止溢出\n",
    "    cm = cm + np.eye(cm.shape[0]);\n",
    "    N = y.shape[0] + cm.shape[0];\n",
    "    n = y.shape[1];\n",
    "    T = np.diagonal(cm);\n",
    "    A = np.trace(cm)/N;\n",
    "    Pk = T/np.sum(cm,axis=0);\n",
    "    P = np.sum(Pk)/n;\n",
    "    Rk = T/np.sum(cm,axis=1).T;\n",
    "    R = np.sum(Rk)/n;\n",
    "    F1 = 2*P*R/(P+R);\n",
    "    if metrics == 'Accuracy':\n",
    "        return A;\n",
    "    if metrics == 'Precision':\n",
    "        return P;\n",
    "    if metrics == 'Recall':\n",
    "        return R;\n",
    "    if metrics == 'F1':\n",
    "        return F1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "names_to_label = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2};\n",
    "label_to_names = {value: key for key, value in names_to_label.items()}\n",
    "df = pd.read_csv('iris.data', header=None)\n",
    "xs = df.iloc[:, :4].values\n",
    "ts = np.array([names_to_label[name] for name in df.iloc[:, -1]])\n",
    "ys = np.zeros((ts.shape[0], 3));\n",
    "ts = ts.reshape(-1,1);\n",
    "# 标签one hot\n",
    "ts = onehot_label(ts);\n",
    "# 样本及标签顺序随机化\n",
    "(train_data,train_label) = random_division(xs,ts);\n",
    "# 对数据的归一化处理\n",
    "max_x = np.max(np.abs(train_data),axis=0);\n",
    "train_data = train_data / max_x;\n",
    "flag_loading = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构定义\n",
    "if not flag_loading == 1:\n",
    "    layers = 5;\n",
    "    nets = [16,64,256,32,3];\n",
    "    in_shape = [4] + nets[:-1];\n",
    "    out_shape = nets;\n",
    "    act_fun = ['relu','relu','relu','sigmoid','softmax'];\n",
    "    loss_func = 'Real_Cross_Entropy';\n",
    "    metrics = 'Accuracy';\n",
    "    x0 = train_data;\n",
    "    n = x0.shape[0];\n",
    "    m = x0.shape[1];\n",
    "    # 模型中间变量初始化\n",
    "    w = [];\n",
    "    b = [];\n",
    "    x = [];\n",
    "    y = [];\n",
    "    delta = [];\n",
    "    for i in range(layers):\n",
    "        wk = np.random.randn(in_shape[i],out_shape[i]);\n",
    "        bk = np.random.randn(1,nets[i]);\n",
    "        w.append(wk);\n",
    "        b.append(bk);\n",
    "        x.append([]);\n",
    "        y.append([]);\n",
    "        delta.append([]);\n",
    "# 保存初始化参数\n",
    "if flag_loading == 0:\n",
    "    flag_loading = 1;\n",
    "    w_init = copy.deepcopy(w);\n",
    "    b_init = copy.deepcopy(b);\n",
    "    x_init = copy.deepcopy(x);\n",
    "    y_init = copy.deepcopy(y);\n",
    "    delta_init = copy.deepcopy(delta);\n",
    "# 加载初始化参数\n",
    "w = copy.deepcopy(w_init);\n",
    "b = copy.deepcopy(b_init);\n",
    "x = copy.deepcopy(x_init);\n",
    "y = copy.deepcopy(y_init);\n",
    "delta = copy.deepcopy(delta_init);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_grandient(rate,dv,v):\n",
    "    r1 = np.max(np.abs(dv));\n",
    "    r2 = np.max(np.abs(v));\n",
    "    res = rate*r2/r1*dv;\n",
    "    return res;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: loss: 0.53 Accuracy:0.95 \n",
      "ep 1: loss: 0.58 Accuracy:0.93 \n",
      "ep 2: loss: 0.54 Accuracy:0.95 \n",
      "ep 3: loss: 0.52 Accuracy:0.95 \n",
      "ep 4: loss: 0.54 Accuracy:0.95 \n",
      "ep 5: loss: 0.53 Accuracy:0.95 \n",
      "ep 6: loss: 0.52 Accuracy:0.95 \n",
      "ep 7: loss: 0.52 Accuracy:0.95 \n",
      "ep 8: loss: 0.52 Accuracy:0.95 \n",
      "ep 9: loss: 0.52 Accuracy:0.95 \n",
      "ep 10: loss: 0.52 Accuracy:0.95 \n",
      "ep 11: loss: 0.52 Accuracy:0.95 \n",
      "ep 12: loss: 0.52 Accuracy:0.95 \n",
      "ep 13: loss: 0.52 Accuracy:0.95 \n",
      "ep 14: loss: 0.52 Accuracy:0.95 \n",
      "ep 15: loss: 0.52 Accuracy:0.95 \n",
      "ep 16: loss: 0.52 Accuracy:0.95 \n",
      "ep 17: loss: 0.52 Accuracy:0.95 \n",
      "ep 18: loss: 0.52 Accuracy:0.95 \n",
      "ep 19: loss: 0.52 Accuracy:0.95 \n",
      "ep 20: loss: 0.52 Accuracy:0.95 \n",
      "ep 21: loss: 0.52 Accuracy:0.95 \n",
      "ep 22: loss: 0.52 Accuracy:0.95 \n",
      "ep 23: loss: 0.52 Accuracy:0.95 \n",
      "ep 24: loss: 0.52 Accuracy:0.95 \n",
      "ep 25: loss: 0.52 Accuracy:0.95 \n",
      "ep 26: loss: 0.52 Accuracy:0.95 \n",
      "ep 27: loss: 0.52 Accuracy:0.95 \n",
      "ep 28: loss: 0.52 Accuracy:0.95 \n",
      "ep 29: loss: 0.52 Accuracy:0.95 \n",
      "ep 30: loss: 0.52 Accuracy:0.95 \n",
      "ep 31: loss: 0.52 Accuracy:0.95 \n",
      "ep 32: loss: 0.52 Accuracy:0.95 \n",
      "ep 33: loss: 0.52 Accuracy:0.95 \n",
      "ep 34: loss: 0.52 Accuracy:0.95 \n",
      "ep 35: loss: 0.52 Accuracy:0.95 \n",
      "ep 36: loss: 0.52 Accuracy:0.95 \n",
      "ep 37: loss: 0.52 Accuracy:0.95 \n",
      "ep 38: loss: 0.52 Accuracy:0.95 \n",
      "ep 39: loss: 0.52 Accuracy:0.95 \n",
      "ep 40: loss: 0.52 Accuracy:0.95 \n",
      "ep 41: loss: 0.52 Accuracy:0.95 \n",
      "ep 42: loss: 0.52 Accuracy:0.95 \n",
      "ep 43: loss: 0.52 Accuracy:0.95 \n",
      "ep 44: loss: 0.52 Accuracy:0.95 \n",
      "ep 45: loss: 0.52 Accuracy:0.95 \n",
      "ep 46: loss: 0.52 Accuracy:0.95 \n",
      "ep 47: loss: 0.52 Accuracy:0.95 \n",
      "ep 48: loss: 0.52 Accuracy:0.95 \n",
      "ep 49: loss: 0.52 Accuracy:0.95 \n",
      "ep 50: loss: 0.52 Accuracy:0.95 \n",
      "ep 51: loss: 0.52 Accuracy:0.95 \n",
      "ep 52: loss: 0.52 Accuracy:0.95 \n",
      "ep 53: loss: 0.52 Accuracy:0.95 \n",
      "ep 54: loss: 0.52 Accuracy:0.95 \n",
      "ep 55: loss: 0.52 Accuracy:0.95 \n",
      "ep 56: loss: 0.52 Accuracy:0.95 \n",
      "ep 57: loss: 0.52 Accuracy:0.95 \n",
      "ep 58: loss: 0.52 Accuracy:0.95 \n",
      "ep 59: loss: 0.52 Accuracy:0.95 \n",
      "ep 60: loss: 0.52 Accuracy:0.95 \n",
      "ep 61: loss: 0.52 Accuracy:0.95 \n",
      "ep 62: loss: 0.52 Accuracy:0.95 \n",
      "ep 63: loss: 0.52 Accuracy:0.95 \n",
      "ep 64: loss: 0.52 Accuracy:0.95 \n",
      "ep 65: loss: 0.52 Accuracy:0.95 \n",
      "ep 66: loss: 0.52 Accuracy:0.95 \n",
      "ep 67: loss: 0.51 Accuracy:0.95 \n",
      "ep 68: loss: 0.51 Accuracy:0.95 \n",
      "ep 69: loss: 0.51 Accuracy:0.95 \n",
      "ep 70: loss: 0.51 Accuracy:0.95 \n",
      "ep 71: loss: 0.51 Accuracy:0.95 \n",
      "ep 72: loss: 0.51 Accuracy:0.95 \n",
      "ep 73: loss: 0.51 Accuracy:0.95 \n",
      "ep 74: loss: 0.51 Accuracy:0.95 \n",
      "ep 75: loss: 0.51 Accuracy:0.95 \n",
      "ep 76: loss: 0.51 Accuracy:0.95 \n",
      "ep 77: loss: 0.51 Accuracy:0.95 \n",
      "ep 78: loss: 0.51 Accuracy:0.95 \n",
      "ep 79: loss: 0.51 Accuracy:0.95 \n",
      "ep 80: loss: 0.51 Accuracy:0.95 \n",
      "ep 81: loss: 0.51 Accuracy:0.95 \n",
      "ep 82: loss: 0.51 Accuracy:0.95 \n",
      "ep 83: loss: 0.51 Accuracy:0.95 \n",
      "ep 84: loss: 0.51 Accuracy:0.95 \n",
      "ep 85: loss: 0.51 Accuracy:0.95 \n",
      "ep 86: loss: 0.51 Accuracy:0.95 \n",
      "ep 87: loss: 0.51 Accuracy:0.95 \n",
      "ep 88: loss: 0.51 Accuracy:0.95 \n",
      "ep 89: loss: 0.51 Accuracy:0.95 \n",
      "ep 90: loss: 0.51 Accuracy:0.95 \n",
      "ep 91: loss: 0.51 Accuracy:0.95 \n",
      "ep 92: loss: 0.51 Accuracy:0.95 \n",
      "ep 93: loss: 0.51 Accuracy:0.95 \n",
      "ep 94: loss: 0.51 Accuracy:0.95 \n",
      "ep 95: loss: 0.51 Accuracy:0.95 \n",
      "ep 96: loss: 0.51 Accuracy:0.95 \n",
      "ep 97: loss: 0.51 Accuracy:0.95 \n",
      "ep 98: loss: 0.51 Accuracy:0.95 \n",
      "ep 99: loss: 0.51 Accuracy:0.95 \n"
     ]
    }
   ],
   "source": [
    "epoches = 100;\n",
    "rate = 5e-4;\n",
    "drate = 2;\n",
    "res_w = [];res_b = [];res_delta = [];res_dw = [];res_db = [];\n",
    "loss = float('inf');\n",
    "min_dloss = 0;\n",
    "for p in range(epoches):\n",
    "    x0 = train_data;\n",
    "    # 正向传播\n",
    "    # 输入层情况\n",
    "    x[0] = np.dot(x0,w[0]) + b[0];\n",
    "    y[0] = func(act_fun[0],x[0]);\n",
    "    # 隐藏层\n",
    "    for k in range(1,layers):\n",
    "        x[k] = np.dot(y[k-1],w[k]) + b[k];\n",
    "        y[k] = func(act_fun[k],x[k]);\n",
    "    \n",
    "    if not type(metrics) == type(None):\n",
    "        y_pre = arg_max(y[k]);\n",
    "        evalue = evaluate(y_pre,train_label,metrics=metrics);\n",
    "    # 反向传播\n",
    "    # res_w.append([]);res_b.append([]);res_dw.append([]);res_db.append([]);res_delta.append([]);\n",
    "    # 输出层情况\n",
    "    delta[k] = gra_func(loss_func,y[k],train_label)*gra_func(act_fun[k],x[k]);\n",
    "    # delta量的归一化处理\n",
    "    # delta[k] = delta[k] / np.max(np.abs(delta[k]));\n",
    "    db = np.sum(delta[k],axis=0).reshape(1,-1);\n",
    "    dw = np.dot(y[k-1].T,delta[k]);\n",
    "    # res_w[p].append(copy.deepcopy(w[k]));res_b[p].append(copy.deepcopy(b[k]));res_delta[p].append(copy.deepcopy(delta[k]));res_dw[p].append(copy.deepcopy(dw));res_db[p].append(copy.deepcopy(db));\n",
    "    \n",
    "    # b[k] -= rate*db;\n",
    "    # w[k] -= rate*dw;\n",
    "    b[k] -= adaptive_grandient(rate,db,b[k]);\n",
    "    w[k] -= adaptive_grandient(rate,dw,w[k]);\n",
    "    k -= 1;\n",
    "    # 隐藏层\n",
    "    while(k>0):\n",
    "        delta[k] = np.dot(delta[k+1],w[k+1].T) * gra_func(act_fun[k],x[k]);\n",
    "        # delta的归一化处理\n",
    "        # delta[k] = delta[k] / np.max(np.abs(delta[k]));\n",
    "        db = np.sum(delta[k],axis = 0).reshape(1,-1);\n",
    "        dw = np.dot(y[k-1].T,delta[k]);\n",
    "        # res_w[p].append(copy.deepcopy(w[k]));res_b[p].append(copy.deepcopy(b[k]));res_delta[p].append(copy.deepcopy(delta[k]));res_dw[p].append(copy.deepcopy(dw));res_db[p].append(copy.deepcopy(db));\n",
    "\n",
    "        b[k] -= adaptive_grandient(rate,db,b[k]);\n",
    "        w[k] -= adaptive_grandient(rate,dw,w[k]);\n",
    "        # b[k] -= rate*db;\n",
    "        # w[k] -= rate*dw;\n",
    "        k -= 1;\n",
    "    \n",
    "    res = func(loss_func,y[-1],train_label);\n",
    "    dloss = loss - res;\n",
    "    loss = res;\n",
    "    if dloss < min_dloss:\n",
    "        rate /= drate;\n",
    "    print('ep {}: loss: {:.2f}'.format(p,loss),end='');\n",
    "    if not type(metrics) == type(None):\n",
    "        print(' {}:{:.2f} '.format(metrics,evalue));\n",
    "    else:\n",
    "        print('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res_dw;\n",
    "abs_max = [];\n",
    "mean_data = [];\n",
    "med_data = [];\n",
    "abs_min = [];\n",
    "for i in range(len(data)):\n",
    "    abs_max.append([]);\n",
    "    mean_data.append([]);\n",
    "    med_data.append([]);\n",
    "    abs_min.append([]);\n",
    "\n",
    "    for k in range(len(data[i])-1,-1,-1):\n",
    "        abs_max[i].append(np.max(np.abs(data[i][k])));\n",
    "        mean_data[i].append(np.mean(data[i][k]));\n",
    "        med_data[i].append(np.median(data[i][k]));\n",
    "        abs_min[i].append(np.min(np.abs(data[i][k])));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.DataFrame(abs_max);\n",
    "writer = pd.ExcelWriter('abs_max.xlsx');\n",
    "data_pd.to_excel(writer,'page_1',float_format='%.6f');\n",
    "writer.save();\n",
    "writer.close();\n",
    "\n",
    "data_pd = pd.DataFrame(mean_data);\n",
    "writer = pd.ExcelWriter('mean_data.xlsx');\n",
    "data_pd.to_excel(writer,'page_1',float_format='%.6f');\n",
    "writer.save();\n",
    "writer.close();\n",
    "\n",
    "data_pd = pd.DataFrame(med_data);\n",
    "writer = pd.ExcelWriter('med_data.xlsx');\n",
    "data_pd.to_excel(writer,'page_1',float_format='%.6f');\n",
    "writer.save();\n",
    "writer.close();\n",
    "\n",
    "data_pd = pd.DataFrame(abs_min);\n",
    "writer = pd.ExcelWriter('abs_min.xlsx');\n",
    "data_pd.to_excel(writer,'page_1',float_format='%.6f');\n",
    "writer.save();\n",
    "writer.close();"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce6f82c3eafde872384b2df90d4ff3d60be8762ce3510564dd0f34869981a8dc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
